---
id: esmo-elcap-llm-2025
title: "ESMO First Guidance on the Safe Use of Large Language Models in Oncology Practice (ELCAP)"
short_title: "ESMO ELCAP 2025"

organization: European Society for Medical Oncology
collaborators: null
country: Global (Europe-led)
url: https://www.esmo.org/guidelines/guidelines-by-topic/elcap-llm-guidance
doi: null
pmid: null
open_access: true

specialty: oncology
guideline_type: clinical-practice
evidence_system: ESMO Consensus
conditions:
  - oncology practice
  - medical AI
  - natural language processing
tags:
  - large language models
  - LLMs
  - AI ethics
  - patient safety
  - medical informatics

publication_date: 2025-10-18
previous_version_date: null
status: current
supersedes: null
superseded_by: null

pdf_path: null
has_pdf: false
last_reviewed: 2025-12-23
---

## Scope
Pioneering 2025 guidance from ESMO providing recommendations for oncologists, researchers, and healthcare institutions on the integration, validation, and ethical use of Large Language Models (LLMs) in clinical decision support and administrative tasks.

## Key Recommendations

### Clinical Decision Support
- **Human Oversight**: LLMs should never be used as a standalone diagnostic or treatment-decision tool. They must function as a supportive tool with final verification by a qualified oncologist.
- **Hallucinations**: Clinicians must be trained to recognize and verify "hallucinations" (plausible but factually incorrect information) generated by LLMs, especially regarding dosing and trial eligibility.

### Patient Privacy and Data Security
- **HIPAA/GDPR Compliance**: Personal Identifiable Information (PII) must never be entered into public or non-secure LLM interfaces.
- **Secure Instances**: Institutions are encouraged to use private, secure, and enterprise-grade instances of LLMs with strict data-sharing controls.

### Clinical Research and Writing
- **Transparency**: Authors using LLMs for manuscript preparation or data analysis must explicitly disclose the tool used and the nature of its contribution in the methods or acknowledgments section.
- **Verification of Citations**: LLM-generated citations must be manually verified for accuracy and relevance.

### Patient Communication
- **Patient-Facing Content**: LLMs can be used to simplify complex medical information for patients, but all materials must be reviewed for clinical accuracy, empathy, and cultural appropriateness by a medical professional.
- **Disclosure**: Patients should be informed if an AI tool was primarily involved in generating their educational materials or summaries.

### Institutional Governance
- **Review Boards**: Institutions should establish AI Ethics or Review Boards to evaluate the implementation of LLM tools within their clinical workflows.
- **Continuous Validation**: Recommends periodic benchmarking of used LLMs against gold-standard clinical datasets to monitor for performance drift.

### Health Equity and Bias
- **Monitoring**: Recognition that LLMs may inherit and amplify biases present in their training data. Clinicians should be aware of potential disparities in the accuracy of LLM outputs for different demographic groups.
---
